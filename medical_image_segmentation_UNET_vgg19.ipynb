{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "medical_image_segmentation_UNET_vgg19.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM7km0TeYk3FsP88CGY3F6h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pmsk98/medical_video_segmentation/blob/main/medical_image_segmentation_UNET_vgg19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaUS8uh6IATU"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZUBdRUdPZjv"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "os.chdir('/content/gdrive/My Drive/경진대회/Kvasir-SEG/')\n",
        "os.getcwd()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq0tQIlhmGxs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYEPkvGcPZtO"
      },
      "source": [
        "import os, cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random, tqdm\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import albumentations as album"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mufo8T1BPZv2"
      },
      "source": [
        "DATA_DIR ='/content/gdrive/My Drive/경진대회/Kvasir-SEG/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drS5i8qiPZy4"
      },
      "source": [
        "!pip install -q -U segmentation-models-pytorch albumentations > /dev/null\n",
        "import segmentation_models_pytorch as smp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqwsvfVBigXb"
      },
      "source": [
        "DATA_DIR ='/content/gdrive/My Drive/경진대회/Kvasir-SEG/'\n",
        "\n",
        "\n",
        "image_list = os.listdir('/content/gdrive/My Drive/경진대회/Kvasir-SEG/images')\n",
        "mask_list = os.listdir('/content/gdrive/My Drive/경진대회/Kvasir-SEG/masks')\n",
        "\n",
        "image_list = sorted(image_list)\n",
        "mask_list = sorted(mask_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl7SdLDhj8o2"
      },
      "source": [
        "metadata_df = pd.DataFrame({'png_image_path':image_list,'png_mask_path':mask_list})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOEu1D9pkJJW"
      },
      "source": [
        "metadata_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GCLrFZIkojN"
      },
      "source": [
        "DATA_DIR ='/content/gdrive/My Drive/경진대회/Kvasir-SEG/images/'\n",
        "\n",
        "\n",
        "metadata_df['png_image_path'] = metadata_df['png_image_path'].apply(lambda img_pth: os.path.join(DATA_DIR, img_pth))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOrFB8Xlko7B"
      },
      "source": [
        "DATA_DIR ='/content/gdrive/My Drive/경진대회/Kvasir-SEG/masks/'\n",
        "\n",
        "\n",
        "metadata_df['png_mask_path'] = metadata_df['png_mask_path'].apply(lambda img_pth: os.path.join(DATA_DIR, img_pth))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdii1DPEkvhG"
      },
      "source": [
        "metadata_df['png_image_path'][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkDgDLaRloPm"
      },
      "source": [
        "metadata_df['png_mask_path'][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MzRLiS6PZ1R"
      },
      "source": [
        "\n",
        "# Shuffle DataFrame\n",
        "metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Perform 90/10 split for train / val\n",
        "valid_df = metadata_df.sample(frac=0.1, random_state=42)\n",
        "train_df = metadata_df.drop(valid_df.index)\n",
        "len(train_df), len(valid_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWJh9o32bJsU"
      },
      "source": [
        "class_dict = pd.read_csv(os.path.join('/content/gdrive/My Drive/경진대회/Kvasir-SEG/class_dict.csv'))\n",
        "# Get class names\n",
        "class_names = class_dict['class_names'].tolist()\n",
        "# Get class RGB values\n",
        "class_rgb_values = class_dict[['r','g','b']].values.tolist()\n",
        "\n",
        "print('All dataset classes and their corresponding RGB values in labels:')\n",
        "print('Class Names: ', class_names)\n",
        "print('Class RGB values: ', class_rgb_values)\n",
        "\n",
        "\n",
        "\n",
        "select_classes = ['background', 'polyp']\n",
        "\n",
        "# Get RGB values of required classes\n",
        "select_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\n",
        "select_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n",
        "\n",
        "print('Selected classes and their corresponding RGB values in labels:')\n",
        "print('Class Names: ', class_names)\n",
        "print('Class RGB values: ', class_rgb_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH-oO0LEbLjB"
      },
      "source": [
        "# helper function for data visualization\n",
        "def visualize(**images):\n",
        "    \"\"\"\n",
        "    Plot images in one row\n",
        "    \"\"\"\n",
        "    n_images = len(images)\n",
        "    plt.figure(figsize=(20,8))\n",
        "    for idx, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n_images, idx + 1)\n",
        "        plt.xticks([]); \n",
        "        plt.yticks([])\n",
        "        # get title from the parameter names\n",
        "        plt.title(name.replace('_',' ').title(), fontsize=20)\n",
        "        plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "# Perform one hot encoding on label\n",
        "def one_hot_encode(label, label_values):\n",
        "    \"\"\"\n",
        "    Convert a segmentation image label array to one-hot format\n",
        "    by replacing each pixel value with a vector of length num_classes\n",
        "    # Arguments\n",
        "        label: The 2D array segmentation image label\n",
        "        label_values\n",
        "        \n",
        "    # Returns\n",
        "        A 2D array with the same width and hieght as the input, but\n",
        "        with a depth size of num_classes\n",
        "    \"\"\"\n",
        "    semantic_map = []\n",
        "    for colour in label_values:\n",
        "        equality = np.equal(label, colour)\n",
        "        class_map = np.all(equality, axis = -1)\n",
        "        semantic_map.append(class_map)\n",
        "    semantic_map = np.stack(semantic_map, axis=-1)\n",
        "\n",
        "    return semantic_map\n",
        "    \n",
        "# Perform reverse one-hot-encoding on labels / preds\n",
        "def reverse_one_hot(image):\n",
        "    \"\"\"\n",
        "    Transform a 2D array in one-hot format (depth is num_classes),\n",
        "    to a 2D array with only 1 channel, where each pixel value is\n",
        "    the classified class key.\n",
        "    # Arguments\n",
        "        image: The one-hot format image \n",
        "        \n",
        "    # Returns\n",
        "        A 2D array with the same width and hieght as the input, but\n",
        "        with a depth size of 1, where each pixel value is the classified \n",
        "        class key.\n",
        "    \"\"\"\n",
        "    x = np.argmax(image, axis = -1)\n",
        "    return x\n",
        "\n",
        "# Perform colour coding on the reverse-one-hot outputs\n",
        "def colour_code_segmentation(image, label_values):\n",
        "    \"\"\"\n",
        "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
        "    # Arguments\n",
        "        image: single channel array where each value represents the class key.\n",
        "        label_values\n",
        "\n",
        "    # Returns\n",
        "        Colour coded image for segmentation visualization\n",
        "    \"\"\"\n",
        "    colour_codes = np.array(label_values)\n",
        "    x = colour_codes[image.astype(int)]\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui9xpA0XbQLS"
      },
      "source": [
        "class EndoscopyDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    \"\"\"CVC-ClinicDB Endoscopic Colonoscopy Dataset. Read images, apply augmentation and preprocessing transformations.\n",
        "    \n",
        "    Args:\n",
        "        df (str): DataFrame containing images / labels paths\n",
        "        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n",
        "        augmentation (albumentations.Compose): data transfromation pipeline \n",
        "            (e.g. flip, scale, etc.)\n",
        "        preprocessing (albumentations.Compose): data preprocessing \n",
        "            (e.g. noralization, shape manipulation, etc.)\n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self, \n",
        "            df,\n",
        "            class_rgb_values=None, \n",
        "            augmentation=None, \n",
        "            preprocessing=None,\n",
        "    ):\n",
        "        self.image_paths = df['png_image_path'].tolist()\n",
        "        self.mask_paths = df['png_mask_path'].tolist()\n",
        "        \n",
        "        self.class_rgb_values = class_rgb_values\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        # read images and masks\n",
        "        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # one-hot-encode the mask\n",
        "        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n",
        "        \n",
        "        # apply augmentations\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        # apply preprocessing\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "            \n",
        "        return image, mask\n",
        "        \n",
        "    def __len__(self):\n",
        "        # return length of \n",
        "        return len(self.image_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY2BYA9YbTGZ"
      },
      "source": [
        "dataset = EndoscopyDataset(train_df, class_rgb_values=select_class_rgb_values)\n",
        "random_idx = random.randint(0, len(dataset)-1)\n",
        "image, mask = dataset[1]\n",
        "\n",
        "visualize(\n",
        "    original_image = image,\n",
        "    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
        "    one_hot_encoded_mask = reverse_one_hot(mask)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4S-76nPbb8X"
      },
      "source": [
        "def get_training_augmentation():\n",
        "    train_transform = [\n",
        "        album.Resize(height=512, width=608, interpolation=cv2.INTER_CUBIC, always_apply=True),\n",
        "        album.HorizontalFlip(p=0.5),\n",
        "        album.VerticalFlip(p=0.5),\n",
        "    ]\n",
        "    return album.Compose(train_transform)\n",
        "\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    # Add sufficient padding to ensure image is divisible by 32\n",
        "    test_transform = [\n",
        "        album.Resize(height=512, width=608, interpolation=cv2.INTER_CUBIC, always_apply=True),\n",
        "    ]\n",
        "    return album.Compose(test_transform)\n",
        "\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "\n",
        "def get_preprocessing(preprocessing_fn=None):\n",
        "    \"\"\"Construct preprocessing transform    \n",
        "    Args:\n",
        "        preprocessing_fn (callable): data normalization function \n",
        "            (can be specific for each pretrained neural network)\n",
        "    Return:\n",
        "        transform: albumentations.Compose\n",
        "    \"\"\"   \n",
        "    _transform = []\n",
        "    if preprocessing_fn:\n",
        "        _transform.append(album.Lambda(image=preprocessing_fn))\n",
        "    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n",
        "        \n",
        "    return album.Compose(_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9FvXQZIovgy"
      },
      "source": [
        "augmented_dataset = EndoscopyDataset(\n",
        "    train_df, \n",
        "    augmentation=get_training_augmentation(),\n",
        "    class_rgb_values=select_class_rgb_values,\n",
        ")\n",
        "\n",
        "random_idx = random.randint(0, len(augmented_dataset)-1)\n",
        "\n",
        "# Different augmentations on image/mask pairs\n",
        "for idx in range(1):\n",
        "    image, mask = augmented_dataset[idx]\n",
        "    visualize(\n",
        "        original_image = image,\n",
        "        ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
        "        one_hot_encoded_mask = reverse_one_hot(mask)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpKI7eD_sy_d"
      },
      "source": [
        "image[1].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2gDar0xs37q"
      },
      "source": [
        "mask[1].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-phGCpcFoxI5"
      },
      "source": [
        "ENCODER = 'vgg19'\n",
        "ENCODER_WEIGHTS = 'imagenet'\n",
        "CLASSES = select_classes\n",
        "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
        "\n",
        "# create segmentation model with pretrained encoder\n",
        "model = smp.Unet(\n",
        "    encoder_name=ENCODER, \n",
        "    encoder_weights=ENCODER_WEIGHTS, \n",
        "    classes=len(CLASSES), \n",
        "    activation=ACTIVATION,\n",
        ")\n",
        "\n",
        "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOil6Z3LpJMp"
      },
      "source": [
        "!pip install -U git+https://github.com/albu/albumentations --no-cache-dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FAXFwZ4ozUi"
      },
      "source": [
        "# Get train and val dataset instances\n",
        "import albumentations.pytorch\n",
        "train_dataset = EndoscopyDataset(\n",
        "    train_df, \n",
        "    augmentation=get_training_augmentation(),\n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    class_rgb_values=select_class_rgb_values,\n",
        ")\n",
        "\n",
        "valid_dataset = EndoscopyDataset(\n",
        "    valid_df, \n",
        "    augmentation=get_validation_augmentation(), \n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    class_rgb_values=select_class_rgb_values,\n",
        ")\n",
        "\n",
        "# Get train and val data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haW32Ehbo1Rw"
      },
      "source": [
        "# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n",
        "TRAINING = True\n",
        "\n",
        "# Set num of epochs\n",
        "EPOCHS = 20\n",
        "\n",
        "# Set device: `cuda` or `cpu`\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# define loss function\n",
        "loss = smp.utils.losses.DiceLoss()\n",
        "\n",
        "# define metrics\n",
        "metrics = [\n",
        "    smp.utils.metrics.IoU(threshold=0.5),\n",
        "]\n",
        "\n",
        "# define optimizer\n",
        "optimizer = torch.optim.Adam([ \n",
        "    dict(params=model.parameters(), lr=0.0001),\n",
        "])\n",
        "\n",
        "# define learning rate scheduler (not used in this NB)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgM4WAadqMeP"
      },
      "source": [
        "if os.path.exists('/content/gdrive/My Drive/경진대회/Kvasir-SEG/best_model_unet.pth'):\n",
        "    model = torch.load('/content/gdrive/My Drive/경진대회/Kvasir-SEG/best_model_unet.pth', map_location=DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3u8lImrp9AW"
      },
      "source": [
        "train_epoch = smp.utils.train.TrainEpoch(\n",
        "    model, \n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    optimizer=optimizer,\n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "valid_epoch = smp.utils.train.ValidEpoch(\n",
        "    model, \n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc1q6LIsp_pc"
      },
      "source": [
        "%%time\n",
        "\n",
        "if TRAINING:\n",
        "\n",
        "    best_iou_score = 0.0\n",
        "    train_logs_list, valid_logs_list = [], []\n",
        "\n",
        "    for i in range(0, EPOCHS):\n",
        "\n",
        "        # Perform training & validation\n",
        "        print('\\nEpoch: {}'.format(i))\n",
        "        train_logs = train_epoch.run(train_loader)\n",
        "        valid_logs = valid_epoch.run(valid_loader)\n",
        "        train_logs_list.append(train_logs)\n",
        "        valid_logs_list.append(valid_logs)\n",
        "\n",
        "        # Save model if a better val IoU score is obtained\n",
        "        if best_iou_score < valid_logs['iou_score']:\n",
        "            best_iou_score = valid_logs['iou_score']\n",
        "            torch.save(model, './best_model_unet.pth')\n",
        "            print('Model saved!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrBrWeiAWqSy"
      },
      "source": [
        "# load best saved model checkpoint from the current run\n",
        "if os.path.exists('/content/gdrive/My Drive/경진대회/Kvasir-SEG/best_model_unet.pth'):\n",
        "    best_model = torch.load('/content/gdrive/My Drive/경진대회/Kvasir-SEG/best_model_unet.pth', map_location=DEVICE)\n",
        "    print('Loaded UNet model from this run.')\n",
        "\n",
        "# load best saved model checkpoint from previous commit (if present)\n",
        "elif os.path.exists('/content/gdrive/My Drive/경진대회/Kvasir-SEG/best_model_unet.pth'):\n",
        "    best_model = torch.load('/content/gdrive/My Drive/경진대회/Kvasir-SEG/best_model_unet.pth', map_location=DEVICE)\n",
        "    print('Loaded UNet model from a previous commit.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQuAiU5KWqXw"
      },
      "source": [
        "# create test dataloader to be used with UNet model (with preprocessing operation: to_tensor(...))\n",
        "test_dataset = EndoscopyDataset(\n",
        "    valid_df, \n",
        "    augmentation=get_validation_augmentation(), \n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    class_rgb_values=select_class_rgb_values,\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset)\n",
        "\n",
        "# test dataset for visualization (without preprocessing augmentations & transformations)\n",
        "test_dataset_vis = EndoscopyDataset(\n",
        "    valid_df,\n",
        "    class_rgb_values=select_class_rgb_values,\n",
        ")\n",
        "\n",
        "# get a random test image/mask index\n",
        "random_idx = random.randint(0, len(test_dataset_vis)-1)\n",
        "image, mask = test_dataset_vis[random_idx]\n",
        "\n",
        "visualize(\n",
        "    original_image = image,\n",
        "    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
        "    one_hot_encoded_mask = reverse_one_hot(mask)\n",
        ")\n",
        "\n",
        "# Notice the images / masks are 1536*1536 because of 18px padding on all sides. \n",
        "# This is to ensure the input image dimensions to UNet model are a multiple of 2 (to account for pooling & transpose conv. operations)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSawDxXZWqdb"
      },
      "source": [
        "sample_preds_folder = '/content/gdrive/My Drive/경진대회/Kvasir-SEG/sample_predictions_unet/'\n",
        "if not os.path.exists(sample_preds_folder):\n",
        "    os.makedirs(sample_preds_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5XMfyoBWql_"
      },
      "source": [
        "for idx in range(len(test_dataset)):\n",
        "\n",
        "    image, gt_mask = test_dataset[idx]\n",
        "    image_vis = test_dataset_vis[idx][0].astype('uint8')\n",
        "    image_vis = cv2.resize(image_vis, (608, 512), interpolation=cv2.INTER_CUBIC)\n",
        "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
        "    # Predict test image\n",
        "    pred_mask = best_model(x_tensor)\n",
        "    pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
        "    # Convert pred_mask from `CHW` format to `HWC` format\n",
        "    pred_mask = np.transpose(pred_mask,(1,2,0))\n",
        "    # Get prediction channel corresponding to foreground\n",
        "    pred_polyp_heatmap = pred_mask[:,:,select_classes.index('polyp')]\n",
        "    pred_mask = colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values)\n",
        "    # Convert gt_mask from `CHW` format to `HWC` format\n",
        "    gt_mask = np.transpose(gt_mask,(1,2,0))\n",
        "    gt_mask = colour_code_segmentation(reverse_one_hot(gt_mask), select_class_rgb_values)\n",
        "    cv2.imwrite(os.path.join(sample_preds_folder, f\"sample_pred_{idx}.png\"), np.hstack([image_vis, gt_mask, pred_mask])[:,:,::-1])\n",
        "    \n",
        "    visualize(\n",
        "        original_image = image_vis,\n",
        "        ground_truth_mask = gt_mask,\n",
        "        predicted_mask = pred_mask,\n",
        "        pred_polyp_heatmap = pred_polyp_heatmap\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1XlEGajZQQo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}